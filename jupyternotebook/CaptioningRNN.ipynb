{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5c0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a8e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Any, Callable, Optional, Tuple, List\n",
    "from collections import defaultdict\n",
    "\n",
    "class CocoCaptionDataset(torchvision.datasets.vision.VisionDataset):\n",
    "        def __init__(self, root, annFile, caption_idx = 1, transform = None, target_transform = None, transforms = None):\n",
    "            super().__init__(root, transforms, transform, target_transform)\n",
    "            from pycocotools.coco import COCO\n",
    "            assert caption_idx < 4\n",
    "            \n",
    "            self.coco = COCO(annFile)\n",
    "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "            \n",
    "            self.caption_idx = caption_idx\n",
    "            self.vocab_to_idx, self. idx_to_vocab = self._build_vocab_to_idx_mapp()\n",
    "            self.idx_captions = self._captions_to_idx()\n",
    "        \n",
    "        def _build_vocab_to_idx_mapp(self, num_words= 1000):\n",
    "            \n",
    "            vocab_count = defaultdict(int)\n",
    "            for id in self.ids:\n",
    "                annotations = self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "                caption = annotations[self.caption_idx]['caption']\n",
    "                \n",
    "                for word in caption.lower().strip('.').split():\n",
    "                    vocab_count[word] += 1\n",
    "                    \n",
    "            special_tokens = [('<NULL>', 0),('<START>', 0),('<END>', 0),('<UNK>', 0)]\n",
    "            ordered = list(sorted(vocab_count.items(), key= lambda item: item[1], reverse=True))\n",
    "            first1KTuple = sorted(ordered[:num_words-len(special_tokens)], key = lambda item: item[0])\n",
    "\n",
    "\n",
    "            vocab_to_idx = {x[0] : i for x, i in zip(special_tokens + first1KTuple, range(num_words))}\n",
    "            idx_to_vocab = [k for k, v in vocab_to_idx.items()]     \n",
    "            return vocab_to_idx, idx_to_vocab\n",
    "        \n",
    "        def _captions_to_idx(self, fix_length= 16):\n",
    "            print('Map caption words to idx!!')\n",
    "            \n",
    "            master = {}\n",
    "            for id in tqdm(self.ids):\n",
    "                annotations = self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "                caption = annotations[self.caption_idx]['caption']\n",
    "                caption = caption.lower().strip('.').split()\n",
    "                idx = [self.vocab_to_idx['<START>']]\n",
    "                for i in range(fix_length):\n",
    "                    \n",
    "                    if i < len(caption):\n",
    "                        try:\n",
    "                            idx.append(self.vocab_to_idx[caption[i]])\n",
    "                        except:\n",
    "                            idx.append(self.vocab_to_idx['<UNK>'])\n",
    "                    elif i == len(caption):\n",
    "                        idx.append(self.vocab_to_idx['<END>'])\n",
    "                    \n",
    "                    else:\n",
    "                        idx.append(self.vocab_to_idx['<NULL>'])\n",
    "                    \n",
    "                    idx_tensor = torch.tensor(idx, dtype=torch.long)\n",
    "                master[id] = idx_tensor\n",
    "            return master\n",
    "\n",
    "        def _load_image(self, id: int) -> Image.Image:\n",
    "            path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
    "            return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "\n",
    "        def _load_target(self, id) -> List[str]:\n",
    "            return self.idx_captions[id]\n",
    "\n",
    "        def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "            id = self.ids[index]\n",
    "            image = self._load_image(id)\n",
    "            target = self._load_target(id)\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                image, target = self.transforms(image, target)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.ids)\n",
    "        \n",
    "        \n",
    "        \n",
    "class CocoCaptionsLT(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, folder_path, batch_size = 64, caption_idx = 1, transformer = None):\n",
    "        '''\n",
    "        Input:\n",
    "          folder_path: A folder that contains both train and validation images and annotation (.json)\n",
    "          caption_idx: Original file contain 4 or 5 captions per image. We will only pick one according to index.\n",
    "          transformers: Pytorch data transforms that will used during preprocess data. \n",
    "                        **All data are prepocessed during set up phase. i.e. no transform are used during training**\n",
    "        '''\n",
    "        assert caption_idx < 4\n",
    "\n",
    "        super().__init__()\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.caption_idx = caption_idx\n",
    "        self.transformer = transformer\n",
    "        \n",
    "        self.train_img_path = os.path.join(folder_path, 'train2017')\n",
    "        self.train_caption_path = os.path.join(folder_path, 'annotations/captions_train2017.json')\n",
    "        self.val_img_path = os.path.join(folder_path,'val2017')\n",
    "        self.val_caption_path = os.path.join(folder_path,'annotations/captions_val2017.json')\n",
    "\n",
    "        if self.transformer is None:\n",
    "            self.transformer = transforms.Compose([\n",
    "                                                    transforms.Resize(112),\n",
    "                                                    transforms.CenterCrop(112),\n",
    "                                                    transforms.ToTensor()\n",
    "                                                    ])\n",
    "\n",
    "        \n",
    "    def prepare_data(self):\n",
    "            \n",
    "        self.train_dataset = CocoCaptionDataset(self.train_img_path, self.train_caption_path, transform = self.transformer)\n",
    "        self.val_dataset = CocoCaptionDataset(self.val_img_path, self.val_caption_path, transform = self.transformer)\n",
    "        \n",
    "        self.vocab_to_idx = self.train_dataset.vocab_to_idx\n",
    "        self.idx_to_vocab = self.train_dataset.idx_to_vocab\n",
    "    \n",
    "    def sample(self, num_samples= 4):\n",
    "        idx = torch.randint(0, len(self.train_dataset), (num_samples,), dtype=torch.int)\n",
    "   \n",
    "        for i in idx:\n",
    "            img, cap = self.train_dataset[i]\n",
    "            plt.imshow(img.permute(1,2,0).numpy())\n",
    "            plt.title(self._decode_captions(cap))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    def _decode_captions(self, captions):\n",
    "        \"\"\"\n",
    "        Decoding caption indexes into words.\n",
    "        Inputs:\n",
    "        - captions: Caption indexes in a tensor of shape (Nx)T.\n",
    "        - idx_to_word: Mapping from the vocab index to word.\n",
    "        Outputs:\n",
    "        - decoded: A sentence (or a list of N sentences).\n",
    "        \"\"\"\n",
    "        singleton = False\n",
    "        if captions.ndim == 1:\n",
    "            singleton = True\n",
    "            captions = captions[None]\n",
    "        decoded = []\n",
    "        N, T = captions.shape\n",
    "        for i in range(N):\n",
    "            words = []\n",
    "            for t in range(T):\n",
    "                word = self.idx_to_vocab[captions[i, t]]\n",
    "                if word != '<NULL>':\n",
    "                    words.append(word)\n",
    "                if word == '<END>':\n",
    "                    break\n",
    "            decoded.append(' '.join(words))\n",
    "        if singleton:\n",
    "            decoded = decoded[0]\n",
    "        return decoded\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        trainDataLoader = torch.utils.data.DataLoader(self.train_dataset, self.batch_size,\n",
    "                                                      shuffle=True, num_workers=8, pin_memory=True)\n",
    "        return trainDataLoader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valDataLoader = torch.utils.data.DataLoader(self.val_dataset, self.batch_size,\n",
    "                                                      shuffle=True, num_workers=8, pin_memory=True)\n",
    "        return valDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dcff00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningRNN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, img_size= 2048, hidden_size= 512, vocab_vec_size = 128, vocab_size= 1000):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backBone = nn.Sequential(*list(model.children())[:-1]) # remove the last classifer\n",
    "        \n",
    "        # freeze backbone parameters\n",
    "        for param in self.backBone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "            \n",
    "            \n",
    "        self.feat_to_h0 = nn.Linear(img_size, hidden_size)\n",
    "            \n",
    "        self.cell = nn.LSTMCell(vocab_vec_size, hidden_size)\n",
    "        \n",
    "        self.wordEmbed = nn.Embedding(vocab_size, vocab_vec_size)\n",
    "        \n",
    "        self.cellOuput_to_vocab = nn.Linear(hidden_size, vocab_size)\n",
    "         \n",
    "        # TODO, information need to know when load data\n",
    "        self.T = 16 # need to figure out when load the dataloader\n",
    "        self.NULL = 0\n",
    "        self.START = 1\n",
    "        \n",
    "            \n",
    "    def _extract_image_features(self, x):   \n",
    "        feat = self.backBone(x)\n",
    "        feat = feat.squeeze()\n",
    "        feat = F.normalize(feat, dim=1) # l2 normalize\n",
    "        return feat\n",
    "    \n",
    "    def _loss(self, x, y):\n",
    "        return F.cross_entropy(x.reshape(-1,x.shape[2]), y.reshape(-1,), ignore_index=self.NULL, reduction='sum') / x.shape[0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B = x.shape[0]\n",
    "         \n",
    "        img_feat = self._extract_image_features(x) # (B, img_size)\n",
    "        \n",
    "        # Initial hidden state as image features\n",
    "        h = self.feat_to_h0(img_feat)\n",
    "        \n",
    "        # Initial cell state as zeros\n",
    "        c = torch.zeros_like(h)\n",
    "        \n",
    "        # Initial start words\n",
    "        words = self.wordEmbed(torch.full((B,), self.START, device=x.device))\n",
    "        \n",
    "        rawVocabScores = []\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            h, c = self.cell(words, (h, c))\n",
    "            \n",
    "            # Affine transform hidden state to vocab size\n",
    "            vocabScore = self.cellOuput_to_vocab(h)\n",
    "            \n",
    "            # Grab the maximum idx of words\n",
    "            predict = torch.argmax(vocabScore, dim=1)\n",
    "            \n",
    "            # saved for outputs\n",
    "            rawVocabScores.append(vocabScore)            \n",
    "            \n",
    "            # update words for the next iteration\n",
    "            words = self.wordEmbed(predict)\n",
    "            \n",
    "        rawVocabScores = torch.stack(rawVocabScores, dim=1)\n",
    "\n",
    "        return rawVocabScores \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        captions_in = y[:,:-1]\n",
    "        captions_out = y[:,1:]\n",
    "        \n",
    "        # Grab embedding of ground truth words\n",
    "        GTwords = self.wordEmbed(captions_in) #(B, sentence_length, word_vec_size)\n",
    "  \n",
    "        # Extract image features\n",
    "        img_feat = self._extract_image_features(x) # (B, img_size)\n",
    "        \n",
    "        # Affine transform image features to match word embeding size\n",
    "        h = self.feat_to_h0(img_feat)\n",
    "        \n",
    "        # initial cell state\n",
    "        c = torch.zeros_like(h)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        # Use image features as initial hidden and cell state\n",
    "        for t in range(self.T):\n",
    "            h, c = self.cell(GTwords[:,t,:], (h, c))\n",
    "            output.append(h)\n",
    "        \n",
    "        output = torch.stack(output, dim=1)\n",
    "        # Affine transform output to vocabulary size\n",
    "        vocabScore = self.cellOuput_to_vocab(output) #(B, sentence_length, vocab_size)\n",
    "        \n",
    "\n",
    "        loss = self._loss(vocabScore, captions_out)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss  \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        captions_out = y[:,1:]\n",
    "        \n",
    "        vocabScore = self(x) # calling forward here\n",
    "\n",
    "        loss = self._loss(vocabScore, captions_out)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def sample(self, x):\n",
    "        \n",
    "        vocabScore = self(x) \n",
    "        \n",
    "        predictWords = torch.argmax(vocabScore, dim=2)\n",
    "        return predictWords\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b438f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/fred/datasets/coco/\"\n",
    "DataModule = CocoCaptionsLT(folder_path, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71b63e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model = CaptioningRNN()\n",
    "trainer = pl.Trainer(gpus = 1, max_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fc1bf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Map caption words to idx!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9903e001723940c4ba54442833f8da47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Map caption words to idx!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc0956438b4b2ab02e8637fc0faf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type       | Params\n",
      "--------------------------------------------------\n",
      "0 | backBone           | Sequential | 23.5 M\n",
      "1 | feat_to_h0         | Linear     | 1.0 M \n",
      "2 | cell               | LSTMCell   | 1.3 M \n",
      "3 | wordEmbed          | Embedding  | 128 K \n",
      "4 | cellOuput_to_vocab | Linear     | 513 K \n",
      "--------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "26.5 M    Total params\n",
      "106.052   Total estimated model params size (MB)\n",
      "/home/fred/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c98fc20b77244b5a3844ec19045d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, DataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106ea67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616fecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
