{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5c0001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T00:47:23.675176Z",
     "start_time": "2021-07-19T00:47:22.389074Z"
    }
   },
   "outputs": [],
   "source": [
    "import comet_ml # need to be import first before torch and pytorch_lightning\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a8e4d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T00:47:23.687632Z",
     "start_time": "2021-07-19T00:47:23.675992Z"
    }
   },
   "outputs": [],
   "source": [
    "class CocoCaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CocoCaptionsLT(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, folder_path, batch_size=64, caption_idx=1, transformer=None):\n",
    "        '''\n",
    "        Input:\n",
    "          folder_path: A folder that contains both train and validation images and annotation (.json)\n",
    "          caption_idx: Original file contain 4 or 5 captions per image. We will only pick one according to index.\n",
    "          transformers: Pytorch data transforms that will used during preprocess data. \n",
    "                        **All data are prepocessed during set up phase. i.e. no transform are used during training**\n",
    "        '''\n",
    "        assert caption_idx < 4\n",
    "\n",
    "        super().__init__()\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.caption_idx = caption_idx\n",
    "        self.transformer = transformer\n",
    "\n",
    "        try:\n",
    "            print(f'Loading coco2017_caption{self.caption_idx}_data_dict')\n",
    "            self.data_dict = torch.load(os.path.join(\n",
    "                folder_path, f'coco2017_caption{self.caption_idx}_data_dict'))\n",
    "        except:\n",
    "            self.data_dict = None\n",
    "            train_img_path = os.path.join(folder_path, 'train2017')\n",
    "            train_caption_path = os.path.join(\n",
    "                folder_path, 'annotations/captions_train2017.json')\n",
    "            val_img_path = os.path.join(folder_path, 'val2017')\n",
    "            val_caption_path = os.path.join(\n",
    "                folder_path, 'annotations/captions_val2017.json')\n",
    "\n",
    "            if self.transformer is None:\n",
    "                self.transformer = transforms.Compose([\n",
    "                    transforms.Resize(112),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "\n",
    "            self.train_dataset = torchvision.datasets.CocoCaptions(\n",
    "                train_img_path, train_caption_path, transform=self.transformer)\n",
    "            self.val_dataset = torchvision.datasets.CocoCaptions(\n",
    "                val_img_path, val_caption_path, transform=self.transformer)\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        if self.data_dict is None:\n",
    "            print('Load fail. Build dataset from scratch. This will take a while!!')\n",
    "            self.vocab_to_idx, self.idx_to_vocab = self._create_vocab_idx_mapping(\n",
    "                self.train_dataset, self.caption_idx)\n",
    "            self.train_dataset = self._tokenize_captions(\n",
    "                self.train_dataset, self.caption_idx, self.vocab_to_idx)\n",
    "            self.val_dataset = self._tokenize_captions(\n",
    "                self.val_dataset, self.caption_idx, self.vocab_to_idx)\n",
    "\n",
    "            self.data_dict = {\"vocab_to_idx\": self.vocab_to_idx,\n",
    "                              \"idx_to_vocab\": self.idx_to_vocab,\n",
    "                              \"train_dataset\": self.train_dataset,\n",
    "                              \"val_dataset\": self.val_dataset,\n",
    "                              'vocab_count': self.vocab_count}\n",
    "            torch.save(self.data_dict, os.path.join(self.folder_path,\n",
    "                       f'coco2017_caption{self.caption_idx}_data_dict'))\n",
    "        else:\n",
    "            print('Load successful.')\n",
    "            self.vocab_to_idx = self.data_dict[\"vocab_to_idx\"]\n",
    "            self.idx_to_vocab = self.data_dict[\"idx_to_vocab\"]\n",
    "            self.train_dataset = self.data_dict['train_dataset']\n",
    "            self.val_dataset = self.data_dict['val_dataset']\n",
    "            self.vocab_count = self.data_dict['vocab_count']\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        trainDataLoader = torch.utils.data.DataLoader(self.train_dataset, self.batch_size,\n",
    "                                                      shuffle=False, num_workers=12, pin_memory=True)\n",
    "        return trainDataLoader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valDataLoader = torch.utils.data.DataLoader(self.val_dataset, self.batch_size,\n",
    "                                                    shuffle=False, num_workers=12, pin_memory=True)\n",
    "        return valDataLoader\n",
    "\n",
    "    def sample(self, num_samples=4):\n",
    "        idx = torch.randint(0, len(self.train_dataset),\n",
    "                            (num_samples,), dtype=torch.int)\n",
    "\n",
    "        for i in idx:\n",
    "            img, cap = self.train_dataset[i]\n",
    "            plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "            plt.title(self._decode_captions(cap))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    def _tokenize_captions(self, dataset, caption_idx, vocab_to_idx, fix_length=16):\n",
    "        '''\n",
    "        Add <'START'> token at the beginning of the caption,\n",
    "        Add <'END'> token at the end of the caption,\n",
    "        Replace unknow words into <'UNK'> token,\n",
    "        Pad caption to a fix length with <'NULL'> tokens\n",
    "        and tanslate all into index and return as a Pytorch dataset\n",
    "\n",
    "        Input:\n",
    "          dataset: dataset which are gonna be used to count words\n",
    "          caption_idx: Each image in original Coco dataset has 4-5 captions. \n",
    "                       We will only use *ONE* of them according to the index.\n",
    "                       Index should be less than 4\n",
    "        Return:\n",
    "          dataset: Pytorch dataset which can be used in dataloader\n",
    "\n",
    "        '''\n",
    "        assert caption_idx < 4\n",
    "        print(\"Start tokenize dataset. This will take a while!!\")\n",
    "\n",
    "        master = []\n",
    "        for image, captions in tqdm(dataset):\n",
    "\n",
    "            caption = captions[caption_idx]\n",
    "            caption = caption.lower().split()\n",
    "            words = [vocab_to_idx['<START>']]\n",
    "            for i in range(fix_length):\n",
    "\n",
    "                if i < len(caption):\n",
    "                    word = caption[i].strip('.')\n",
    "                    try:\n",
    "                        words.append(vocab_to_idx[word])\n",
    "                    except:\n",
    "                        words.append(vocab_to_idx['<UNK>'])\n",
    "\n",
    "                elif i == len(caption):\n",
    "                    words.append(vocab_to_idx['<END>'])\n",
    "\n",
    "                else:\n",
    "                    words.append(vocab_to_idx['<NULL>'])\n",
    "\n",
    "            master.append((image, torch.tensor(words)))\n",
    "        return CocoCaptionDataset(master)\n",
    "\n",
    "    def _create_vocab_idx_mapping(self, dataset, caption_idx, num_words=1000):\n",
    "        '''\n",
    "        Use the first 5k examples to map vocabulary to idx and\n",
    "        grab the first 1k most frequent words in the datasets\n",
    "\n",
    "        Input:\n",
    "          dataset: dataset which are gonna be used to count words\n",
    "          caption_idx: Each image in original Coco dataset has 4-5 captions. \n",
    "                       We will only use *ONE* of them according to the index.\n",
    "                       Index should be less than 4\n",
    "\n",
    "        Output:\n",
    "          vocab_to_idx: dictionary\n",
    "          idx_to_vocab: list\n",
    "        '''\n",
    "        assert caption_idx < 4\n",
    "        from collections import defaultdict\n",
    "        print('Start creating vocab index mapping. This will take several minutes!!')\n",
    "\n",
    "        smaller_dataset, _ = torch.utils.data.random_split(\n",
    "            dataset, [5000, len(dataset)-5000])\n",
    "        vocab_count = defaultdict(int)\n",
    "        for _, captions in tqdm(smaller_dataset):\n",
    "            caption = captions[caption_idx]\n",
    "            caption = caption.lower().split()\n",
    "\n",
    "            for word in caption:\n",
    "                word = word.strip('.')\n",
    "                if len(word) > 0:\n",
    "                    vocab_count[word] += 1\n",
    "\n",
    "        special_tokens = [('<NULL>', 0), ('<START>', 0),\n",
    "                          ('<END>', 0), ('<UNK>', 0)]\n",
    "        ordered = list(sorted(vocab_count.items(),\n",
    "                       key=lambda item: item[1], reverse=True))\n",
    "        first1KTuple = sorted(\n",
    "            ordered[:num_words-len(special_tokens)], key=lambda item: item[0])\n",
    "\n",
    "        vocab_to_idx = {x[0]: i for x, i in zip(\n",
    "            special_tokens + first1KTuple, range(num_words))}\n",
    "        idx_to_vocab = [k for k, v in vocab_to_idx.items()]\n",
    "\n",
    "        for i in range(num_words):\n",
    "            word = idx_to_vocab[i]\n",
    "            assert vocab_to_idx[word] == i, 'Vocab mapping failed, words are missed match'\n",
    "\n",
    "        self.vocab_count = vocab_count\n",
    "        return vocab_to_idx, idx_to_vocab\n",
    "\n",
    "    def _decode_captions(self, captions):\n",
    "        \"\"\"\n",
    "        Decoding caption indexes into words.\n",
    "        Inputs:\n",
    "        - captions: Caption indexes in a tensor of shape (Nx)T.\n",
    "        - idx_to_word: Mapping from the vocab index to word.\n",
    "        Outputs:\n",
    "        - decoded: A sentence (or a list of N sentences).\n",
    "        \"\"\"\n",
    "        singleton = False\n",
    "        if captions.ndim == 1:\n",
    "            singleton = True\n",
    "            captions = captions[None]\n",
    "        decoded = []\n",
    "        N, T = captions.shape\n",
    "        for i in range(N):\n",
    "            words = []\n",
    "            for t in range(T):\n",
    "                word = self.idx_to_vocab[captions[i, t]]\n",
    "                if word != '<NULL>':\n",
    "                    words.append(word)\n",
    "                if word == '<END>':\n",
    "                    break\n",
    "            decoded.append(' '.join(words))\n",
    "        if singleton:\n",
    "            decoded = decoded[0]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dcff00d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T00:47:23.695968Z",
     "start_time": "2021-07-19T00:47:23.688404Z"
    }
   },
   "outputs": [],
   "source": [
    "class CaptioningRNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr=3e-4, img_size=2048, time_stamps=16, hidden_size=512, vocab_vec_size=128, vocab_size=1000):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hparams = {'lr': lr}\n",
    "        self.T = time_stamps\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        # remove the last classifer\n",
    "        self.backBone = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        # unfreeze backbone parameters\n",
    "        for param in self.backBone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.feat_to_h0 = nn.Linear(img_size, hidden_size)\n",
    "\n",
    "        self.cell = nn.LSTMCell(vocab_vec_size, hidden_size)\n",
    "\n",
    "        self.wordEmbed = nn.Embedding(vocab_size, vocab_vec_size)\n",
    "\n",
    "        self.cellOuput_to_vocab = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # information need to know when load data\n",
    "        self.NULL = 0\n",
    "        self.START = 1\n",
    "\n",
    "    def _extract_image_features(self, x):\n",
    "        feat = self.backBone(x)\n",
    "        feat = feat.squeeze()\n",
    "        return feat\n",
    "\n",
    "    def _loss(self, x, y):\n",
    "        return F.cross_entropy(x.reshape(-1, x.shape[2]), y.reshape(-1,), ignore_index=self.NULL, reduction='sum') / x.shape[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B = x.shape[0]\n",
    "\n",
    "        img_feat = self._extract_image_features(x)  # (B, img_size)\n",
    "\n",
    "        # Initial hidden state as image features\n",
    "        h = self.feat_to_h0(img_feat)\n",
    "\n",
    "        # Initial cell state as zeros\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        # Initial start words\n",
    "        words = self.wordEmbed(torch.full((B,), self.START, device=self.device))\n",
    "\n",
    "        rawVocabScores = []\n",
    "\n",
    "        for t in range(self.T):\n",
    "            h, c = self.cell(words, (h, c))\n",
    "\n",
    "            # Affine transform hidden state to vocab size\n",
    "            vocabScore = self.cellOuput_to_vocab(h)\n",
    "\n",
    "            # Grab the maximum idx of words\n",
    "            predict = torch.argmax(vocabScore, dim=1)\n",
    "\n",
    "            # update words for the next iteration\n",
    "            words = self.wordEmbed(predict)\n",
    "\n",
    "            # saved for outputs\n",
    "            rawVocabScores.append(vocabScore)\n",
    "\n",
    "        out = torch.stack(rawVocabScores, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        captions_in = y[:, :-1]\n",
    "        captions_out = y[:, 1:]\n",
    "\n",
    "        # Grab embedding of ground truth words\n",
    "        # (B, sentence_length, word_vec_size)\n",
    "        GTwords = self.wordEmbed(captions_in)\n",
    "\n",
    "        # Extract image features\n",
    "        img_feat = self._extract_image_features(x)  # (B, img_size)\n",
    "\n",
    "        # Affine transform image features to match word embeding size\n",
    "        h = self.feat_to_h0(img_feat)\n",
    "\n",
    "        # initial cell state\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        # Use image features as initial hidden and cell state\n",
    "        for t in range(self.T):\n",
    "            h, c = self.cell(GTwords[:, t, :], (h, c))\n",
    "            output.append(h)\n",
    "\n",
    "        output = torch.stack(output, dim=1)\n",
    "        # Affine transform output to vocabulary size\n",
    "        # (B, sentence_length, vocab_size)\n",
    "        vocabScore = self.cellOuput_to_vocab(output)\n",
    "\n",
    "        loss = self._loss(vocabScore, captions_out)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        captions_out = y[:, 1:]\n",
    "\n",
    "        vocabScore = self(x)  # calling forward here\n",
    "\n",
    "        loss = self._loss(vocabScore, captions_out)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams['lr'])\n",
    "\n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        self.logger.finalize(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b438f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T00:48:05.666069Z",
     "start_time": "2021-07-19T00:47:24.492495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading coco2017_caption1_data_dict\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/fred/datasets/coco/\"\n",
    "DataModule = CocoCaptionsLT(folder_path, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71b63e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T00:48:11.422681Z",
     "start_time": "2021-07-19T00:48:11.049722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "model = CaptioningRNN()\n",
    "trainer = pl.Trainer(gpus = 1, precision=16, overfit_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc1bf1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-19T00:48:14.728Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type       | Params\n",
      "--------------------------------------------------\n",
      "0 | backBone           | Sequential | 23.5 M\n",
      "1 | feat_to_h0         | Linear     | 1.0 M \n",
      "2 | cell               | LSTMCell   | 1.3 M \n",
      "3 | wordEmbed          | Embedding  | 128 K \n",
      "4 | cellOuput_to_vocab | Linear     | 513 K \n",
      "--------------------------------------------------\n",
      "26.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.5 M    Total params\n",
      "106.052   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9a19f470c34725aa2560298f36bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/fred/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/fred/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/fred/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, DataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e917b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(CaptioningRNN().backBone.parameters())[:-50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CaptioningRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616fecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bfe97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
